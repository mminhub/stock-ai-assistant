import streamlit as st
import feedparser
import yfinance as yf
import requests
import re
import time
from datetime import datetime
import urllib.parse
from bs4 import BeautifulSoup

# ==============================================================================
# [1] ì„¤ì •
# ==============================================================================
st.set_page_config(page_title="Strategic AI Partner", layout="wide")

# API í‚¤ í™•ì¸
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("ğŸš¨ Secrets ì„¤ì •ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
    st.stop()

API_KEY = st.secrets["GOOGLE_API_KEY"]

# ì‚¬ìš©ìë‹˜ì´ ì„±ê³µí•˜ì…¨ë‹¤ëŠ” 1.5 ëª¨ë¸ì„ ë©”ì¸ìœ¼ë¡œ ì”ë‹ˆë‹¤
RELAY_MODELS = ["gemini-1.5-flash", "gemini-1.5-flash-8b"]

# ==============================================================================
# [2] AI ë° ë°ì´í„° ì—”ì§„ (ë””ë²„ê¹… ê°•í™”íŒ)
# ==============================================================================
def clean_text(text):
    if not text: return ""
    return re.sub(r'[\[\]\{\}\"]', '', text).strip()

def call_ai_relay(prompt):
    error_logs = [] 
    for model in RELAY_MODELS:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={API_KEY}"
        headers = {'Content-Type': 'application/json'}
        data = {"contents": [{"parts": [{"text": prompt}]}]}
        
        try:
            res = requests.post(url, headers=headers, json=data, timeout=30)
            
            if res.status_code == 200:
                return res.json()['candidates'][0]['content']['parts'][0]['text'], model
            
            elif res.status_code == 429:
                # [ìˆ˜ì •] 429 ì—ëŸ¬ë„ ë¡œê·¸ì— ë‚¨ê¹€!
                msg = f"[{model}] 429 ê³¼ë¶€í•˜ (Too Many Requests) - ì„œë²„ê°€ ë°”ì¨"
                error_logs.append(msg)
                time.sleep(1)
                continue
            
            else:
                # ê¸°íƒ€ ì—ëŸ¬
                msg = f"[{model}] Error {res.status_code}: {res.text}"
                error_logs.append(msg)
                continue
                
        except Exception as e:
            error_logs.append(f"[{model}] í†µì‹  ì˜¤ë¥˜: {str(e)}")
            continue
            
    # ì—¬ê¸°ê¹Œì§€ ì™”ë‹¤ëŠ” ê±´ ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í–ˆë‹¤ëŠ” ëœ»
    return None, "\n".join(error_logs)

@st.cache_data(ttl=600)
def fetch_market_data():
    # 1. ì£¼ì‹ ë°ì´í„°
    try:
        tickers = ['^TNX', '^VIX', 'BTC-USD', 'GC=F', '^GSPC', '^IXIC']
        df = yf.download(tickers, period="5d", progress=False)['Close'].ffill()
        last = df.iloc[-1]
        prev = df.iloc[-2]
        chg = ((last - prev) / prev) * 100
    except:
        last, chg = None, None

    # 2. ë‰´ìŠ¤ ë°ì´í„° (ê²€ìƒ‰ì–´ ë‹¨ìˆœí™”)
    # êµ¬ê¸€ ë‰´ìŠ¤ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ì–´ë¥¼ ì•„ì£¼ ë‹¨ìˆœí•˜ê²Œ ë³€ê²½
    rss_url = "https://news.google.com/rss/search?q=Economy+Finance+Bitcoin&hl=en-US&gl=US&ceid=US:en"
    
    try:
        feed = feedparser.parse(rss_url)
        if not feed.entries:
            return last, chg, [] # ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            
        scored_news = []
        for e in feed.entries[:5]: # ê·¸ëƒ¥ ìµœì‹  5ê°œ ê°€ì ¸ì˜´ (ì ìˆ˜ ë¡œì§ ìƒëµí•˜ì—¬ ì—ëŸ¬ ìµœì†Œí™”)
            e.title = clean_text(e.title)
            scored_news.append(e)
        
        return last, chg, scored_news
        
    except:
        return last, chg, []

def get_article_content(link):
    try:
        res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=4)
        soup = BeautifulSoup(res.content, 'html.parser')
        text = ' '.join([p.get_text() for p in soup.find_all('p')])
        if len(text) > 200: return text[:3000]
    except:
        pass
    return "ì›ë¬¸ ì ‘ì† ë¶ˆê°€"

# ==============================================================================
# [3] UI ë¡œì§
# ==============================================================================
PROMPT_BRIEFING = """
ROLE: Investor.
TASK: Analyze the news below in KOREAN.
FORMAT:
[MARKET SCORE] (0-100)
[UPCOMING EVENTS] (3 events)
[MARKET VIEW] (1 sentence)
[TRENDING ASSETS] (3 assets)
[NEWS ANALYSIS]
1. ACTION: (Buy/Sell/Hold) | REASON: ...
"""

PROMPT_DEEP = "Analyze in KOREAN.\nACTION: [Buy/Sell/Hold]\nSUMMARY: -Fact\nRISK: -Risk"

def parse_section(text, header):
    try:
        pattern = re.escape(header) + r"(.*?)(?=\n\[|$)"
        match = re.search(pattern, text, re.DOTALL)
        return match.group(1).strip() if match else ""
    except:
        return ""

def main():
    st.title("â˜• Strategic AI Partner")
    
    if 'briefing_data' not in st.session_state:
        status = st.info("ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        
        last, chg, news = fetch_market_data()
        
        # [í•µì‹¬] ë‰´ìŠ¤ê°€ 0ê°œë©´ AI í˜¸ì¶œí•˜ì§€ ë§ê³  ë©ˆì¶¤ (ì—ëŸ¬ ë°©ì§€)
        if not news:
            status.error("âŒ êµ¬ê¸€ ë‰´ìŠ¤ê°€ ì´ ì„œë²„ì˜ ì ‘ì†ì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤. (ë‰´ìŠ¤ê°€ 0ê°œì…ë‹ˆë‹¤)")
            st.warning("íŒ: ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, Yahoo Finance ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤.")
            st.stop()
            
        st.session_state['market_raw'] = (last, chg, news)
        
        news_txt = "\n".join([f"[{i+1}] {n.title}" for i, n in enumerate(news)])
        
        ai_res, error_log = call_ai_relay(f"{PROMPT_BRIEFING}\n{news_txt}")
        
        if ai_res:
            st.session_state['briefing_data'] = ai_res
            st.success("âœ… ì™„ë£Œ")
            time.sleep(1)
            status.empty()
        else:
            status.error("ë¶„ì„ ì‹¤íŒ¨! (ì•„ë˜ ì—ëŸ¬ ë‚´ìš©ì„ ë³´ì„¸ìš”)")
            st.code(error_log) # ì´ì œ ì—¬ê¸°ì— 429ì¸ì§€ ë­”ì§€ ëœ¹ë‹ˆë‹¤!
            st.stop()

    # ê²°ê³¼ í™”ë©´ (ì´ì „ê³¼ ë™ì¼)
    last, chg, news = st.session_state.get('market_raw', (None, None, []))
    briefing = st.session_state.get('briefing_data', "")

    if last is not None:
        cols = st.columns(6)
        metrics = [("US 10Y", '^TNX'), ("VIX", '^VIX'), ("S&P 500", '^GSPC'), 
                   ("Nasdaq", '^IXIC'), ("BTC", 'BTC-USD'), ("Gold", 'GC=F')]
        for i, (l, k) in enumerate(metrics):
            cols[i].metric(l, f"{last.get(k,0):,.2f}", f"{chg.get(k,0):.2f}%")

    st.divider()

    # íŒŒì‹± ë° ì¶œë ¥
    score_txt = parse_section(briefing, "[MARKET SCORE]")
    view_txt = parse_section(briefing, "[MARKET VIEW]")
    events_txt = parse_section(briefing, "[UPCOMING EVENTS]")
    trending_txt = parse_section(briefing, "[TRENDING ASSETS]")
    
    c1, c2 = st.columns([1, 3])
    with c1:
        st.metric("Risk Score", score_txt[:3] if score_txt else "50")
    with c2:
        st.info(view_txt if view_txt else "ë¶„ì„ ë‚´ìš© ì—†ìŒ")
        
    with st.expander("ğŸ“… ì¼ì • & ğŸš€ íŠ¸ë Œë“œ", expanded=True):
        st.write(events_txt)
        st.divider()
        st.write(trending_txt)

    st.divider()
    for i, n in enumerate(news):
        st.markdown(f"**{i+1}. {n.title}**")
        st.caption(f"[ì›ë¬¸]({n.link})")
        if st.button("ì •ë°€ ë¶„ì„", key=f"d_{i}"):
            # ì •ë°€ ë¶„ì„ ë¡œì§ (ê°„ì†Œí™”)
            body = get_article_content(n.link)
            det, err = call_ai_relay(f"{PROMPT_DEEP}\n{body}")
            if det: st.info(det)
            else: st.error(err)
        st.divider()

    if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
        del st.session_state['briefing_data']
        st.rerun()

if __name__ == "__main__":
    main()